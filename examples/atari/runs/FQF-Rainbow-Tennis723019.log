Observations shape: (4, 84, 84)
Actions shape: 18
3136
device is cuda
18 0.1
3136
Dueling is True
FullQuantileFunctionRainbow(
  (preprocess): DQN(
    (net): Sequential(
      (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
      (1): ReLU(inplace=True)
      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
      (3): ReLU(inplace=True)
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))
      (5): ReLU(inplace=True)
      (6): Flatten(start_dim=1, end_dim=-1)
    )
  )
  (last): MLP(
    (model): Sequential(
      (0): Linear(in_features=3136, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=18, bias=True)
    )
  )
  (embed_model): CosineEmbeddingNetwork(
    (net): Sequential(
      (0): Linear(in_features=64, out_features=3136, bias=True)
      (1): ReLU()
    )
  )
  (advantage_net): Sequential(
    (0): NoisyLinear()
    (1): ReLU(inplace=True)
    (2): NoisyLinear()
  )
  (value_net): Sequential(
    (0): NoisyLinear()
    (1): ReLU(inplace=True)
    (2): NoisyLinear()
  )
)
Noisy is True
Using PER
PER as buffer
Epoch #1: test_reward: -24.000000 ± 0.000000, best_reward: -24.000000 ± 0.000000 in #0
Epoch #2: test_reward: -24.000000 ± 0.000000, best_reward: -24.000000 ± 0.000000 in #0
Epoch #3: test_reward: -23.800000 ± 0.400000, best_reward: -23.800000 ± 0.400000 in #3
Epoch #4: test_reward: -21.300000 ± 6.603787, best_reward: -21.300000 ± 6.603787 in #4
Epoch #5: test_reward: -3.800000 ± 2.821347, best_reward: -3.800000 ± 2.821347 in #5
Epoch #6: test_reward: -10.600000 ± 9.068627, best_reward: -3.800000 ± 2.821347 in #5
Epoch #7: test_reward: -3.500000 ± 3.232646, best_reward: -3.500000 ± 3.232646 in #7
Epoch #8: test_reward: -7.100000 ± 5.769749, best_reward: -3.500000 ± 3.232646 in #7
Epoch #9: test_reward: -8.200000 ± 1.249000, best_reward: -3.500000 ± 3.232646 in #7
Epoch #10: test_reward: -8.300000 ± 1.004988, best_reward: -3.500000 ± 3.232646 in #7
Epoch #11: test_reward: -8.700000 ± 1.100000, best_reward: -3.500000 ± 3.232646 in #7
Epoch #12: test_reward: -7.400000 ± 1.113553, best_reward: -3.500000 ± 3.232646 in #7
Epoch #13: test_reward: -5.000000 ± 3.255764, best_reward: -3.500000 ± 3.232646 in #7
Epoch #14: test_reward: -9.700000 ± 6.664083, best_reward: -3.500000 ± 3.232646 in #7
Epoch #15: test_reward: -13.100000 ± 6.977822, best_reward: -3.500000 ± 3.232646 in #7
Epoch #16: test_reward: -17.100000 ± 7.529276, best_reward: -3.500000 ± 3.232646 in #7
Epoch #17: test_reward: -7.300000 ± 4.859012, best_reward: -3.500000 ± 3.232646 in #7
Epoch #18: test_reward: -6.600000 ± 2.764055, best_reward: -3.500000 ± 3.232646 in #7
Epoch #19: test_reward: -9.300000 ± 5.568662, best_reward: -3.500000 ± 3.232646 in #7
Epoch #20: test_reward: -4.400000 ± 3.231099, best_reward: -3.500000 ± 3.232646 in #7
Epoch #21: test_reward: -11.000000 ± 6.115554, best_reward: -3.500000 ± 3.232646 in #7
Epoch #22: test_reward: -8.600000 ± 3.261901, best_reward: -3.500000 ± 3.232646 in #7
Epoch #23: test_reward: -10.800000 ± 4.142463, best_reward: -3.500000 ± 3.232646 in #7
Epoch #24: test_reward: -10.300000 ± 5.001000, best_reward: -3.500000 ± 3.232646 in #7
Epoch #25: test_reward: -17.900000 ± 6.347440, best_reward: -3.500000 ± 3.232646 in #7
Epoch #26: test_reward: -12.400000 ± 6.651316, best_reward: -3.500000 ± 3.232646 in #7
Epoch #27: test_reward: -7.400000 ± 1.280625, best_reward: -3.500000 ± 3.232646 in #7
Epoch #28: test_reward: -7.700000 ± 0.781025, best_reward: -3.500000 ± 3.232646 in #7
Epoch #29: test_reward: -8.200000 ± 0.748331, best_reward: -3.500000 ± 3.232646 in #7
Epoch #30: test_reward: -7.600000 ± 2.009975, best_reward: -3.500000 ± 3.232646 in #7
Epoch #31: test_reward: -5.400000 ± 3.720215, best_reward: -3.500000 ± 3.232646 in #7
Epoch #32: test_reward: -20.400000 ± 6.216108, best_reward: -3.500000 ± 3.232646 in #7
Epoch #33: test_reward: -9.400000 ± 7.735632, best_reward: -3.500000 ± 3.232646 in #7
Epoch #34: test_reward: -6.300000 ± 2.685144, best_reward: -3.500000 ± 3.232646 in #7
Epoch #35: test_reward: -6.700000 ± 2.238303, best_reward: -3.500000 ± 3.232646 in #7
Epoch #36: test_reward: -1.300000 ± 0.640312, best_reward: -1.300000 ± 0.640312 in #36
Epoch #37: test_reward: -2.600000 ± 2.764055, best_reward: -1.300000 ± 0.640312 in #36
Epoch #38: test_reward: -1.300000 ± 1.100000, best_reward: -1.300000 ± 0.640312 in #36
Epoch #39: test_reward: -3.000000 ± 3.065942, best_reward: -1.300000 ± 0.640312 in #36
Epoch #40: test_reward: -2.600000 ± 3.006659, best_reward: -1.300000 ± 0.640312 in #36
Epoch #41: test_reward: -2.400000 ± 2.905168, best_reward: -1.300000 ± 0.640312 in #36
Epoch #42: test_reward: -1.300000 ± 0.640312, best_reward: -1.300000 ± 0.640312 in #36
Epoch #43: test_reward: -1.100000 ± 0.700000, best_reward: -1.100000 ± 0.700000 in #43
Epoch #44: test_reward: -1.100000 ± 0.538516, best_reward: -1.100000 ± 0.700000 in #43
Epoch #45: test_reward: -8.200000 ± 2.271563, best_reward: -1.100000 ± 0.700000 in #43
Epoch #46: test_reward: -1.700000 ± 1.004988, best_reward: -1.100000 ± 0.700000 in #43
Epoch #47: test_reward: -1.100000 ± 0.830662, best_reward: -1.100000 ± 0.700000 in #43
Epoch #48: test_reward: -2.900000 ± 2.662705, best_reward: -1.100000 ± 0.700000 in #43
Epoch #49: test_reward: -0.900000 ± 0.300000, best_reward: -0.900000 ± 0.300000 in #49
Epoch #50: test_reward: -1.100000 ± 0.538516, best_reward: -0.900000 ± 0.300000 in #49
Epoch #51: test_reward: -0.700000 ± 0.900000, best_reward: -0.700000 ± 0.900000 in #51
Epoch #52: test_reward: -1.200000 ± 0.400000, best_reward: -0.700000 ± 0.900000 in #51
Epoch #53: test_reward: -1.000000 ± 0.774597, best_reward: -0.700000 ± 0.900000 in #51
Epoch #54: test_reward: -1.600000 ± 2.537716, best_reward: -0.700000 ± 0.900000 in #51
Epoch #55: test_reward: -1.300000 ± 0.640312, best_reward: -0.700000 ± 0.900000 in #51
Epoch #56: test_reward: -1.000000 ± 0.774597, best_reward: -0.700000 ± 0.900000 in #51
Epoch #57: test_reward: -1.100000 ± 0.300000, best_reward: -0.700000 ± 0.900000 in #51
Epoch #58: test_reward: -1.100000 ± 0.300000, best_reward: -0.700000 ± 0.900000 in #51
Epoch #59: test_reward: -1.300000 ± 0.458258, best_reward: -0.700000 ± 0.900000 in #51
Epoch #60: test_reward: -1.300000 ± 0.640312, best_reward: -0.700000 ± 0.900000 in #51
Epoch #61: test_reward: -1.200000 ± 0.400000, best_reward: -0.700000 ± 0.900000 in #51
Epoch #62: test_reward: -1.200000 ± 0.400000, best_reward: -0.700000 ± 0.900000 in #51
Epoch #63: test_reward: -1.100000 ± 0.300000, best_reward: -0.700000 ± 0.900000 in #51
Epoch #64: test_reward: -0.900000 ± 0.300000, best_reward: -0.700000 ± 0.900000 in #51
Epoch #65: test_reward: -1.000000 ± 0.000000, best_reward: -0.700000 ± 0.900000 in #51
Epoch #66: test_reward: -1.100000 ± 0.700000, best_reward: -0.700000 ± 0.900000 in #51
Epoch #67: test_reward: -0.900000 ± 0.300000, best_reward: -0.700000 ± 0.900000 in #51
Epoch #68: test_reward: -0.700000 ± 0.900000, best_reward: -0.700000 ± 0.900000 in #51
Epoch #69: test_reward: -1.200000 ± 0.400000, best_reward: -0.700000 ± 0.900000 in #51
Epoch #70: test_reward: -1.100000 ± 0.538516, best_reward: -0.700000 ± 0.900000 in #51
Epoch #71: test_reward: -0.900000 ± 0.700000, best_reward: -0.700000 ± 0.900000 in #51
Epoch #72: test_reward: -2.000000 ± 1.788854, best_reward: -0.700000 ± 0.900000 in #51
Epoch #73: test_reward: -1.200000 ± 0.400000, best_reward: -0.700000 ± 0.900000 in #51
Epoch #74: test_reward: -1.300000 ± 0.458258, best_reward: -0.700000 ± 0.900000 in #51
Epoch #75: test_reward: -1.100000 ± 0.300000, best_reward: -0.700000 ± 0.900000 in #51
Epoch #76: test_reward: -8.200000 ± 0.748331, best_reward: -0.700000 ± 0.900000 in #51
Epoch #77: test_reward: -1.300000 ± 1.004988, best_reward: -0.700000 ± 0.900000 in #51
Epoch #78: test_reward: -1.400000 ± 0.489898, best_reward: -0.700000 ± 0.900000 in #51
Epoch #79: test_reward: -0.900000 ± 0.300000, best_reward: -0.700000 ± 0.900000 in #51
Epoch #80: test_reward: -3.000000 ± 4.381780, best_reward: -0.700000 ± 0.900000 in #51
Epoch #81: test_reward: -1.300000 ± 0.640312, best_reward: -0.700000 ± 0.900000 in #51
Epoch #82: test_reward: -1.000000 ± 0.000000, best_reward: -0.700000 ± 0.900000 in #51
Epoch #83: test_reward: -1.100000 ± 0.300000, best_reward: -0.700000 ± 0.900000 in #51
Epoch #84: test_reward: -1.500000 ± 0.806226, best_reward: -0.700000 ± 0.900000 in #51
Epoch #85: test_reward: -1.100000 ± 0.300000, best_reward: -0.700000 ± 0.900000 in #51
Epoch #86: test_reward: -1.000000 ± 0.000000, best_reward: -0.700000 ± 0.900000 in #51
Epoch #87: test_reward: -1.000000 ± 0.000000, best_reward: -0.700000 ± 0.900000 in #51
Epoch #88: test_reward: -1.000000 ± 0.000000, best_reward: -0.700000 ± 0.900000 in #51
Epoch #89: test_reward: -1.000000 ± 0.000000, best_reward: -0.700000 ± 0.900000 in #51
Epoch #90: test_reward: -1.100000 ± 0.300000, best_reward: -0.700000 ± 0.900000 in #51
Epoch #91: test_reward: -1.000000 ± 0.000000, best_reward: -0.700000 ± 0.900000 in #51
Epoch #92: test_reward: -1.000000 ± 0.447214, best_reward: -0.700000 ± 0.900000 in #51
Epoch #93: test_reward: -1.200000 ± 0.600000, best_reward: -0.700000 ± 0.900000 in #51
Epoch #94: test_reward: -2.200000 ± 2.315167, best_reward: -0.700000 ± 0.900000 in #51
Epoch #95: test_reward: -1.000000 ± 0.000000, best_reward: -0.700000 ± 0.900000 in #51
Epoch #96: test_reward: -1.200000 ± 0.600000, best_reward: -0.700000 ± 0.900000 in #51
Epoch #97: test_reward: -1.000000 ± 0.000000, best_reward: -0.700000 ± 0.900000 in #51
Epoch #98: test_reward: -1.200000 ± 0.400000, best_reward: -0.700000 ± 0.900000 in #51
Epoch #99: test_reward: -1.100000 ± 0.300000, best_reward: -0.700000 ± 0.900000 in #51
Epoch #100: test_reward: -2.100000 ± 2.343075, best_reward: -0.700000 ± 0.900000 in #51
InfoStats(gradient_step=1000000,
          best_reward=-0.7,
          best_reward_std=0.9,
          train_step=10000000,
          train_episode=601,
          test_step=25231840,
          test_episode=1010,
          timing=TimingStats(total_time=118800.79109835625,
                             train_time=60985.89499115944,
                             train_time_collect=23132.635182142258,
                             train_time_update=36493.33448386192,
                             test_time=57814.89610719681,
                             update_speed=163.9723414971545))
Setup test envs ...
Testing agent ...
CollectStats
----------------------------------------
{   'collect_speed': 434.5380385447502,
    'collect_time': 621.3495161533356,
    'lens': array([27000, 27000, 27000, 27000, 27000, 27000, 27000, 27000, 27000,
       27000]),
    'lens_stat': {'max': 27000.0, 'mean': 27000.0, 'min': 27000.0, 'std': 0.0},
    'n_collected_episodes': 10,
    'n_collected_steps': 270000,
    'returns': array([-2., -1., -3., -1., -1., -1., -3., -1., -1., -1.]),
    'returns_stat': {   'max': -1.0,
                        'mean': -1.5,
                        'min': -3.0,
                        'std': 0.806225774829855}}
